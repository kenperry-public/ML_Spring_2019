{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "\\newcommand{idxb}{\\mathbf{i}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( {#1} \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "import cnn_helper\n",
    "%aimport cnn_helper\n",
    "cnnh = cnn_helper.CNN_Helper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overview\n",
    "\n",
    "We have seen that Neural Networks are capable of performing many tasks very well.\n",
    "\n",
    "One surprising aspect of this\n",
    "- we have not *directed* the Neural Network on how to achieve the task\n",
    "- the task is achieved by minimization of a Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have seen that it the *potential* to be a Universal Function approximatator\n",
    "- implementing the function defined implicitly\n",
    "- by the empirical distribution of input/output pairs\n",
    "- represented by the labeled training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But is there any way to gain insight into what is happening within the layers of a Neural Network ?\n",
    "\n",
    "That is\n",
    "- given the many synthetic features created by the Neural Network\n",
    "- can we  discover/interpret what is the meaning of a particular feature ?\n",
    "\n",
    "That is the topic of this module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Interpretation: The first layer\n",
    "\n",
    "It is relatively easy to understand the features created by the first layer\n",
    "- they involve the dot product of an input and some weights\n",
    "- matches inputs $\\x$ against weights (pattern) $\\w$\n",
    "\n",
    "So we can understand the feature\n",
    "- if we understand the pattern\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Inputs with only a feature dimension\n",
    "\n",
    "For examples that have only feature dimensions\n",
    "- the pattern is just a vector of feature values\n",
    "- of length equal to the length of the input example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A Dense Layer has a pattern\n",
    "- that exactly identifies the \"ideal\" input (highest dot product)\n",
    "\n",
    "Recall the $10$ patterns from our simple Logistic Regression Classifier for the $10$ MNIST digits\n",
    "- these are the \"idealized\" digits\n",
    "\n",
    "<br>\n",
    "<table>\n",
    "    <th><center><strong>Patterns for each of the 10 MNIST digits</strong></center></th>\n",
    "    <tr>\n",
    "        <td><img src=\"images/mnist_patterns.png\" width=80%></td>\n",
    "    </tr>\n",
    "</table>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inputs with non-feature dimensions as well as a feature dimension\n",
    "\n",
    "But we also allow examples to have a \"shape\"\n",
    "- *non-feature* dimensions\n",
    "\n",
    "For example\n",
    "- an image has $2$ non-feature dimensions: row and column\n",
    "- in addition to a feature dimensions: e.g., $3$ features: Red, Green, Blue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall our terminology when dealing with examples having $N \\ge 1$ non-feature dimensions\n",
    "- an element is a vector with only a feature dimension\n",
    "- we can index an element by a vector of length $N$ in\n",
    "$$\n",
    "[1:\\dim_{1}] \\times[1:\\dim_{2}] \\times \\ldots [1:\\dim_{N} ]\n",
    "$$\n",
    "- an index identifies a specific *location* in the non-feature dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The patterns are $(N +1)$ dimensional\n",
    "- one feature dimensions of length $n$, which is also the number of input features\n",
    "- $N$ feature dimensions\n",
    "    - each of length $f$\n",
    "    - which is smaller than the length of the corresponding non-feature dimension of the input example\n",
    "    \n",
    "The output of the  match with a single pattern is a *feature map*\n",
    "- $N$-dimensional: matches the lengths of the input non-feature dimensions\n",
    "- a measure of the strength of the pattern's match with the sub-region centered at each location    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So the pattern \n",
    "- identifies an \"ideal\" sub-region in the input example\n",
    "\n",
    "To illustrate\n",
    "- we show the patterns a CNN layer appearing in layer $1$ of a NN\n",
    "- there are $n_{(1)} = 96$ patterns\n",
    "- each pattern is $(7 \\times 7 \\times n_{(0)})$\n",
    "    - $n_{(0)} = 3$ are the number of input channels\n",
    "\n",
    "Each square is a kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <center><strong>Layer 1 kernels</strong></center>\n",
    "    <tr>\n",
    "        <td><img src=\"images/img_on_page_-004-112.jpg\" width=600></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Attribution: https://arxiv.org/pdf/1311.2901.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The \"patterns\" being recognized by these kernels seem to represent\n",
    "- Lines, in various orientations\n",
    "- Colors\n",
    "- Shading\n",
    "\n",
    "We interpret Layer $1$ as trying to construct synthetic features representing these simple concepts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Beyond the first layer\n",
    "\n",
    "Examining weights beyond the first layer presents difficulties\n",
    "- the patterns are matched against outputs of layer $\\ll \\gt 0$\n",
    "- we only know what the features are for layer $0$\n",
    "    - visually recognizable\n",
    "    \n",
    "So we can identify a pattern but can't assign a meaning to the inputs that are being matched.\n",
    "\n",
    "We will have to come up with ways of interpreting synthetic features\n",
    "- that do no involve interpreting the patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Probing\n",
    "\n",
    "One way to gain insight is by *probing*\n",
    "- choose one feature somewhere in the Neural Network\n",
    "- try to discover Layer $0$ inputs\n",
    "- that causes this feature to assume high (positive or negative) values\n",
    "\n",
    "We call the values produced at a feature in response to inputs the feature's *activations*.\n",
    "\n",
    "To eliminate ambiguity, we will write\n",
    "$$\\y_{\\llp,k} |_{  \\y_{(0)} = \\x^\\ip  }$$\n",
    "\n",
    "to denote the activation when the Layer $0$ input is $\\x^\\ip$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If  \n",
    "- we identify a property $\\mathcal{P}$ common to all the inputs resulting in High values\n",
    "- we can interpret the feature as being a detector for $\\mathcal{P}$\n",
    "\n",
    "The common property may not be easy to discern\n",
    "- semantics: meaning\n",
    "- rather than surface: appearance\n",
    "\n",
    "For example\n",
    "- there may be a neuron in some layer\n",
    "- that acts as a \"smile detector\"\n",
    "    - triggering only on inputs containing humans that are smiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To be more precise:\n",
    "\n",
    "Given a multi-layer Sequential Neural Network\n",
    "- choose one feature at some layer to probe: $\\y_{\\llp,k}$\n",
    "\n",
    "We are interested in the output values (called *activations*) of this feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <th><center><strong>Interpreting neuron for feature k in layer l</strong></center></th>\n",
    "    <tr>\n",
    "        <td><img src=\"images/interp.png\" width=80%></td>\n",
    "    </tr>\n",
    "</table>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When the layer $\\ll$ output has only feature-dimensions\n",
    "- the selected feature is a scalar\n",
    "\n",
    "for instance, a `Dense` layer:\n",
    "\n",
    "<br>\n",
    "<table>\n",
    "    <center><strong>Dense layer: $\\y_\\llp$: selecting a neuron to probe</strong></center>\n",
    "    <tr>\n",
    "        <td><center><strong>Dense layer: $\\y_\\llp$</strong></center></td>\n",
    "        <td><center><strong>Dense layer, one neuron selected: $\\y_{\\llp,j}$</strong></center></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/layer.png\"></td>\n",
    "        <td><img src=\"images/layer_select.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But when layer $\\ll$ has $N \\ge 1$ non-feature dimensions \n",
    "- the selected feature is really a *feature map*\n",
    "- with dimensions matching the non-feature dimensions of the layer input\n",
    "$$\n",
    "(\\dim_{1} \\times \\dim_{2} \\times \\ldots \\dim_{N} )\n",
    "$$\n",
    "\n",
    "So \n",
    "there are $\\prod_{i=1}^N { \\dim_{i} }$ values (one per location) in the feature map\n",
    "- rather than a single scalar value\n",
    "- as in the case of layer outputs with only a feature dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<br>\n",
    "<table>\n",
    "    <center><strong>Convolutional layer: $\\y_\\llp$: selecting a feature map to probe</strong></center>\n",
    "    <tr>\n",
    "        <td><center><strong>Layer w/non-feature dimensions: $\\y_\\llp$</strong></center></td>\n",
    "        <td><center><strong>Layer w/non-feature dimensions, one element selected: $\\y_{\\llp,j}$</strong></center></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/layer_w_2d_elements.png\"></td>\n",
    "        <td><img src=\"images/layer_w_2d_elements_select.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In such a case\n",
    "- we reduce each feature map (with non-feature dimensions)\n",
    "- to a scalar\n",
    "- using a Pooling operation to eliminate the non-feature dimensions\n",
    "    - for example: Global Max Pooling\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<br>\n",
    "<table>\n",
    "    <center><strong>Convolutional layer: $\\y_\\llp$: selecting a feature map to probe<br>Global Pooling</strong></center>\n",
    "    <tr>\n",
    "        <td><center><strong>Layer w/non-feature dimensions: $\\y_\\llp$</strong></center></td>\n",
    "        <td><center><strong>Layer w/non-feature dimensions, <br>pooled, one element selected: $\\y_{\\llp,j}$</strong></center></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/layer_w_2d_elements.png\"></td>\n",
    "        <td><img src=\"images/layer_w_2d_elements_pool.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus, Probing \n",
    "- examines the activation of a feature\n",
    "- where the activation is represented\n",
    "- by a single scalar value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximally Activating Examples\n",
    "\n",
    "This method identifies\n",
    "- a subset $S$ of the training examples\n",
    "$$\n",
    "S \\subset \\X\n",
    "$$\n",
    "- that produces high activations for the selected feature $\\y_{\\llp,k}$\n",
    "\n",
    "Hence, this method is called *Maximally Activating Examples*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The method is quite simple\n",
    "- pass each input example $\\x^\\ip$ to the network\n",
    "- measure the resulting activation of the selected feature\n",
    "$$\\y_{\\llp,k} |_{  \\y_{(0)} = \\x^\\ip  }$$\n",
    "- rank the $m$ resulting activations\n",
    "$$ \\{ i_1, \\ldots, i_m \\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Classify \n",
    "    - the $K$ highest (positive) magnitude activations as High\n",
    "    - the $K$ highest (negative) magnitude activations as Low\n",
    "\n",
    "\n",
    "i | $\\y_{\\llp,k}$ | class\n",
    ":--|:--|:--|\n",
    "1 | 7.1 | \n",
    "2 | -100.2 | Low\n",
    "3 | - 6.3 |\n",
    "$\\vdots$\n",
    "234 | 1000.4 | High\n",
    "$\\vdots$\n",
    "$m$ | 45.6 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then the $K$ Maximally Activating examples for $\\y_{\\llp,k}$ are defined as\n",
    "- the $K$ examples with highest rank (classified as High)\n",
    "\n",
    "$$\n",
    "\\text{MaxAct}_{\\llp,k, K} = \\{ \\x^{(i_1)}, \\ldots, \\x^{(i_N)} \\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We then try\n",
    "- via Intuition, Experiment\n",
    "- to identify the property $\\mathcal{P}$\n",
    "- that is unique among $\\X$ to the examples in \n",
    "$\n",
    "\\text{MaxAct}_{\\llp,k, K} = \\{ \\x^{(i_1)}, \\ldots, \\x^{(i_N)} \\}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Probing the  Classifier Head\n",
    "\n",
    "Applying the Maximally Activating Examples technique to the head \n",
    "layer $L$  is particularly useful\n",
    "\n",
    "For a Classifier Head:\n",
    "$$\\y_{(L),k} |_{  \\y_{(0)} = \\x^\\ip  }$$\n",
    "\n",
    "- is the probability (or pre-probability \"logit\")\n",
    "- that example $\\x^\\ip$ is in Class $k$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can use Maximally Activation examples on a Head feature\n",
    "- to identify inputs\n",
    "- that are most/least confidently classified as being in Class $k$\n",
    "\n",
    "Here we apply the technique to a restricted subset $\\X' \\subset \\X$ \n",
    "of input images of digits that have label \"8\"\n",
    "$$\n",
    "\\X' = \\{ \\x^\\ip \\, | \\, \\y^\\ip = 8 \\,\\, \\text{where } 1 \\le i \\le m \\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <center><strong>MNIST CNN maximally activating 8's</strong></center>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/best_worst_8.png\"></td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Interesting !  Do we have a problem with certain 8's ?\n",
    "\n",
    "Much lower probability when\n",
    "- 8 is thin versus thick\n",
    "- tilted left versus right\n",
    "\n",
    "So although our goal was interpretation, this technique may be useful for Error Analysis as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Occlusion\n",
    "\n",
    "Maximally activating inputs are very coarse: they identify concepts at the level of entire input.\n",
    "- when the inputs have non-feature dimensions\n",
    "- Global Pooling compresses *all* the locations to a single scalar\n",
    "- losing information about the sub-region having the property\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There is a simple technique called *Occlusion* \n",
    "- that enables us to find a sub-region of a *particular input* $\\x^\\ip$\n",
    "- that is responsible for the activation \n",
    "$$\\y_{(L),k} |_{  \\y_{(0)} = \\x^\\ip  }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is similar in concept to Convolution applied to Layer $0$ (the example)\n",
    "\n",
    "In Convolution, we take a filter \n",
    "- with $N$ non-feature dimensions\n",
    "- each of length $f$\n",
    "- and $n_{(0)}$ features\n",
    "\n",
    "and compute the dot product of the filter with the sub-region of $\\x^\\ip$ centered at each location.\n",
    "- resulting in a feature map with identical non-feature dimensions as the input\n",
    "$$\n",
    "\\dim_{1} \\times \\dim_{2} \\times \\ldots \\dim_{N} \n",
    "$$\n",
    "- measuring the strength of the match of the filter and sub-region at each location\n",
    "    - for each filter/kernel in the Convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In Occlusion\n",
    "- the sub-region of $\\x^\\ip$ centered at each location\n",
    "- has all its values changed to an extreme value\n",
    "    - equivalent to \"hiding\" the sub-region\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Rather than computing the dot product at each location, Occlusion produces\n",
    "- a feature map (*Occlusion Sensitivity map*) with identical non-feature dimensions as the input\n",
    "- measuring *the change in* the probability $\\y_{(L),k}$\n",
    "    - from un-occluded  $\\y_{(L),k}|_{  \\y_{(0)} = \\x^\\ip  }$\n",
    "    - to the value of $\\y_{(L),k}$ when the location is the center of the occluded region\n",
    "    \n",
    "It is the sensitivity of $\\y_{(L),k}|_{  \\y_{(0)} = \\x^\\ip  }$ to being occluded at each location.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For inputs with non-feature dimensions\n",
    "$$\n",
    "\\dim_{1} \\times \\dim_{2} \\times \\ldots \\dim_{N} \n",
    "$$\n",
    "the Occlusion sensitivity Map has the same non-feature dimensions\n",
    "- just like Convolution with a single kernel/filter\n",
    "\n",
    "Thus the non-feature dimensions of the input and the sensitivity map are identical.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Below is an example for an image with label: Afghan Hound.\n",
    "\n",
    "It would seem that this feature recognizes faces.\n",
    "- activation  drops (blue = cold) when the faces are occluded\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr><td colspan=\"2\"><center><strong>Occlusion Sensitivity Map</strong></center></td></tr>\n",
    "        <th><center>Input image</center></th>\n",
    "        <th><center>Activation of one filter at layer 5</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/img_on_page_-007-139.png\" width=300\"></td>\n",
    "        <td><img src=\"images/img_on_page_-007-148.png\" width=300></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Attribution: https://arxiv.org/pdf/1311.2901.pdf#page=7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Occlusion Experiment 1:  Head Layer  logit on MNIST digit Classification\n",
    "The following figure shows\n",
    "- some of the occluded locations in the feature map\n",
    "- of a particular example $\\x^\\ip$ representing digit \"8\"\n",
    "- with the  proportional change in $\\y_{(L),8}$ indicated at the top of the occluded input\n",
    "- for a NN performing MNIST digit classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<br>\n",
    "<table>\n",
    "    <th><center>Occlusion: Relative decrease in probability of being \"8\"</center></th>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"images/occlude_8_4_1.png\" width=600>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Not what we expected !  \n",
    "\n",
    "The mere presence of the square changes the classification probability\n",
    "greatly\n",
    "- even when we are not occluding what we believed to be the most important sub-regions of $\\x^\\ip$\n",
    "    - the \"pinched waist\" of the 8.\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This suggest that the NN performs Classification \n",
    "- in a way different than what we might have directed to using a Procedural Program\n",
    "- perhaps extreme locations\n",
    "    - are used to recognize other digits\n",
    "    - so the \"bright\" occlusion mask confuses the Classifier\n",
    "    \n",
    "We might want to use Data Augmentation to correct the Classifier\n",
    "- adding noise to inputs, preserving the label\n",
    "- to immunize the Classifier from bright spots at extreme locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Occlusion Experiment 2: How does an  ImageNet Classifier work\n",
    "\n",
    "ImageNet was a competition (important historically in the evolution of Neural Networks)\n",
    "- classification of images\n",
    "- from among 1000 different classes\n",
    "    - 200 different types of dogs and cats !\n",
    "\n",
    "[Zieler and Fergus](https://arxiv.org/pdf/1311.2901.pdf) have some interesting Occlusion results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Occlusion Sensitivity map we used as illustration above comes from this paper\n",
    "- Interpretation of Layer 5 feature: face detector\n",
    "\n",
    "<table>\n",
    "    <tr><td colspan=\"2\"><center><strong>Occlusion Sensitivity Map</strong></center></td></tr>\n",
    "        <th><center>Input image</center></th>\n",
    "        <th><center>Activation of one filter at layer 5</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/img_on_page_-007-139.png\" width=300\"></td>\n",
    "        <td><img src=\"images/img_on_page_-007-148.png\" width=300></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Attribution: https://arxiv.org/pdf/1311.2901.pdf#page=7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The fact that we have discovered a \"face detector\" is interesting.\n",
    "- Faces *are not* one of the 1000 possible labels\n",
    "- Perhaps this non-label feature is necessary\n",
    "    - to assist in creating features that *do identify* labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example\n",
    "- there is evidence that many Classifiers have features that recognize Letter Characters (e.g., A-Z)\n",
    "    - not one of the 1000 classes\n",
    "- which may, in turn\n",
    "    - help to identify \"Book\", which is of the 1000 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The results of probing\n",
    "- the logit of the class \"Afghan Hound\"\n",
    "    - the correct label for the input image\n",
    "- is very interesting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr><td colspan=\"2\"><center><strong>Occlusion Sensitivity Map</strong></center></td></tr>\n",
    "        <th><center>Input image</center></th>\n",
    "        <th><center>Change in logit for \"Afghan hound\"</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/img_on_page_-007-139.png\" width=300\"></td>\n",
    "        <td><img src=\"images/img_on_page_-007-145.png\" width=300></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Attribution: https://arxiv.org/pdf/1311.2901.pdf#page=7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Occluding the dog causes a big drop (blue: cold) in probability of correct classification\n",
    "- as expected\n",
    "\n",
    "But occluding each face *increases the probability* (red: hot) of correct classification !\n",
    "- Perhaps the presence of a face is suggestive of an alternative class\n",
    "    - removing the input signal for the alternative class results in a more confident prediction for the correct clas\n",
    "- Even though \"face\" is not itself a class\n",
    "\n",
    "\n",
    "Occlusion has helped us learn something unexpected about the workings of the Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Saliency maps\n",
    "\n",
    "Each location in the Occlusion Sensitivity map reflects\n",
    "- a change in $\\y_{\\llp,k}$\n",
    "- give a fairly big change\n",
    "    - occlusion replaces pixels with an extreme value\n",
    "- in a region of $\\y_{(0)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can compute a more traditional sensitivity via the derivative\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\y_{\\llp,k}}{\\partial \\y_{(0)}} \\, {\\Big |}_{\\y_{(0)} = \\x^\\ip}\n",
    "$$\n",
    "\n",
    "Each location in this derivative (same non-feature dimensions as $\\y_{(0)}$) reflects\n",
    "- a change in $\\y_{\\llp,k}$\n",
    "- for an infinitesimal change\n",
    "- in a single location in $\\y_{(0)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is called a *Saliency Map*\n",
    "- when input has non-feature dimensions\n",
    "- the Saliency Map has the same non-feature dimensions\n",
    "$$\n",
    "\\dim_{1} \\times \\dim_{2} \\times \\ldots \\dim_{N} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Saliency Maps, when applied to a Head Layer logit $k$\n",
    "- explains the influence of each location in the input $\\y_{(0)}$\n",
    "- on the classification of the input as being in class $k$\n",
    "\n",
    "Hence, they are useful for explaining the output of a NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Understanding a non-head layer via Saliency Maps\n",
    "\n",
    "Saliency Maps are also useful for explaining features in non-head layers.\n",
    "\n",
    "Recall that the Saliency Map and Input have the same non-feature dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Saliency map for a shallow layer\n",
    "\n",
    "Below are a collection of Saliency Maps for\n",
    "some feature in Layer 2 of an ImageNet Classifier.\n",
    "- maps for $9$ **different input examples**\n",
    "    - the examples with largest activation in the feature map\n",
    "\n",
    "All $9$ examples appear to be eye-balls.\n",
    "\n",
    "It would seem this Layer $2$ feature is recognizing eye balls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The diagram can be confusing\n",
    "- they are for $9$ **different input examples**\n",
    "- the non-feature dimensions seem to be for a sub-region (a *patch*) of the input, rather than the entire input\n",
    "    - just the eye, not the rest of the image\n",
    "    \n",
    "We will explain after presenting the diagram.\n",
    "\n",
    "As a first pass\n",
    "- these are the $9$ examples that stimulated the feature most strongly\n",
    "    - hence, may be useful for interpreting what the feature is\n",
    "- on the left is a saliency map for a sub-region (*patch*) of the input\n",
    "- on the right is the corresponding patch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><strong>Saliency Maps and Corresponding Patches<br>Single Layer 2 Feature Map<br>On multiple input images</strong></center>\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images/ZF_p4_115_row10_col3_mag.png\"></td>\n",
    "        <td><img src=\"images/ZF_p4_115_row10_col3_patch_mag.png\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td colspan=2><center>Layer 2 Feature Map (Row 10, col 3).</center></td>\n",
    "    </tr>\n",
    "</table>\n",
    "Attribution: https://arxiv.org/abs/1311.2901#page=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Explaining why the diagram has \"small\" maps and patches**\n",
    "\n",
    "Why are the Saliency Maps and corresponding patches restricted to sub-regions of the input ?\n",
    "- i.e., smaller than\n",
    "$$\n",
    "\\dim_{1} \\times \\dim_{2} \\times \\ldots \\dim_{N} \n",
    "$$\n",
    "\n",
    "Recall that the multiple locations in the layer are reduced to a single value\n",
    "- the max, when using Max Pooling for the summarization\n",
    "- so the Saliency map is the change of a *single location* $\\y_{\\llp,\\text{idx}, k}$ in $\\y_{\\llp,k}$ \n",
    "$$\n",
    "\\frac{\\partial \\y_{\\llp,\\text{idx}, k}}{\\partial \\y_{(0)}} \\, {\\Big |}_{\\y_{(0)} = \\x^\\ip}\n",
    "$$\n",
    "    - where $\\text{idx}$ the location of the *max*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In a NN with multiple CNN layers, \n",
    "- the [*receptive field*](CNN_Receptive_Field.ipynb)\n",
    "- is the input **sub-region** that affects a single location in a layer\n",
    "-  the dimensions of the sub-region grows with the depth (i.e., layer number $\\ll$) of the layer    \n",
    "\n",
    "So, in a shallow layer (i.e., Layer $2$ in our diagram)\n",
    "- the receptive field for *any* location\n",
    "- is less than the full input\n",
    "    - very small: only slightly larger than $f$, the size of a side of the filter/kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus, the non-feature dimensions of the Saliency Map for a shallow layer (e.g., layer 2 in the diagram)\n",
    "- is much smaller than\n",
    "$$\n",
    "\\dim_{1} \\times \\dim_{2} \\times \\ldots \\dim_{N} \n",
    "$$\n",
    "- because the receptive field for $\\text{idx}$, the location of the max in layer $\\ll$\n",
    "- is small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Saliency map for a deeper layer \n",
    "\n",
    "As we go deeper into the network\n",
    "- the size of the receptive field grows in a NN with successive CNN layers\n",
    "- the representations become more complex\n",
    "    - perhaps because of the larger receptive field\n",
    "    - perhaps just because they are combinations of more complex representations\n",
    "        - their layer inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In Layer 5, the feature whose map we show\n",
    "- may be recognizing \"smiling faces\"\n",
    "    - note the high (red) sensitivity\n",
    "    - to lips and cheeks\n",
    "\n",
    "<br>\n",
    "<center><strong>Saliency Maps and Corresponding Patches<br>Single Layer 5 Feature Map<br>On 9 Maximally Activating Input images</strong></center>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images/ZF_p4_118_row11_col1_mag.png\"></td>\n",
    "        <td><img src=\"images/ZF_p4_118_row11_col1_patch_mag.png\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td colspan=2><center>Layer 5 ? Feature Map (Row 11, col 1).</center></td>\n",
    "    </tr>\n",
    "</table>\n",
    "Attribution: https://arxiv.org/abs/1311.2901\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing the Saliency Map\n",
    "\n",
    "Computing a Saliency Map is easy\n",
    "- a simple variant of Back Propagation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall the definition of the Loss Gradient in Back Propagation\n",
    "$$\\loss'_\\llp = \\frac{\\partial \\loss}{\\partial \\y_\\llp}$$ \n",
    "\n",
    "and it's recursive update\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "\\loss'_{(\\ll-1)} & = & \\frac{\\partial \\loss}{\\partial \\y_{(\\ll-1)}} \\\\\n",
    "         & = & \\frac{\\partial \\loss}{\\partial \\y_\\llp} \\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)}} \\\\\n",
    "         & = & \\loss'_\\llp \\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)}}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To compute Saliency Maps\n",
    "- replace $\\loss$ with $\\y_{\\llp,k}$\n",
    "- so the \"loss gradient\" is now the \"saliency gradient\"\n",
    "$$\n",
    "\\loss'_{(\\ll ')} = \\frac{\\partial \\y_{\\llp,k}}{\\partial \\y_{(\\ll ')}}\n",
    "$$\n",
    "    - we use the index $\\ll$ to denote the layer of the feature map\n",
    "    - thus, we are forced to use $\\ll '$ in the subscript of $\\loss'$ to avoid conflict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Substituting $\\ll' = 0$:\n",
    "$$\\loss'_{(0)} = \\frac{\\partial \\y_{\\llp,k}}{\\partial \\y_{(0)}}$$\n",
    "\n",
    "we get the derivative defining the Saliency Map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Guided Back Propagation\n",
    "\n",
    "Our ultimate purpose is to try to *interpret* the meaning of a synthetic feature.\n",
    "\n",
    "The \"true\" mathematical derivative of the Saliency Map\n",
    "- is sometimes sacrificed\n",
    "- in order to enhance the interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[Zeiler and Fergus]( https://arxiv.org/abs/1311.2901) (and similar related papers) modify Back propagation \n",
    "- In an attempt to get better intuition as to which input features most affect $\\y_{\\llp,k}$\n",
    "- For example: ignore the *sign* of the derivatives as they flow backwards\n",
    "    - Look for strong positive or negative influences, not caring which\n",
    "\n",
    "This is called *Guided Back propagation*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Video: interactive interpretation of features\n",
    "\n",
    "There is a nice video by [Yosinski](https://youtu.be/AgkfIQ4IGaM) which examines the behavior of\n",
    "a Neural Network's layers on video images rather than stills.\n",
    "- using several of the techniques we describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
