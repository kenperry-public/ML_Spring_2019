{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( #1 \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training Neural Networks: Practical advice\n",
    "\n",
    "[Andrej Karpathy](https://karpathy.ai/) has an excellent [blog post](https://karpathy.github.io/2019/04/25/recipe/)\n",
    "that conveys much practical wisdom for successfully training Neural Networks.\n",
    "\n",
    "It was written back in 2019 and has the feel of advice from someone who has hand-coded\n",
    "Neural Networks from scratch, rather than someone using higher-level toolkits (e.g., Keras).\n",
    "\n",
    "But: you learn a lot building Neural Networks from scratch (in fact, he has a current free [course](https://karpathy.ai/zero-to-hero.html) (using Pytorch) that does just that.\n",
    "\n",
    "In this module, we will attempt to distill some valuable points from the blog ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Neural training fails silently](https://karpathy.github.io/2019/04/25/recipe/#2-neural-net-training-fails-silently)\n",
    "\n",
    "\n",
    "When writing a program using an imperative programming language (e.g., Python)\n",
    "- failure mode is apparent: run-time error, exception, etc.\n",
    "\n",
    "When creating a Neural Network\n",
    "- failures are often silent\n",
    "- they compute *something*, but not necessarily the something that you desire.  \n",
    "    - manifesting in a large Loss\n",
    "    \n",
    "This makes it hard to debug.\n",
    "\n",
    "But there are some practical steps you can take to minimize the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Become one with the data](https://karpathy.github.io/2019/04/25/recipe/#1-become-one-with-the-data)\n",
    "\n",
    "This is essentially the same as our Exploratory Data Analysis step of our Recipe.\n",
    "- but with more intensity than most of us devote\n",
    "\n",
    "Some key quotes:\n",
    "- I like to spend copious amount of time (**measured in units of hours**) scanning through thousands of examples, understanding their distribution and looking for patterns. \n",
    "\n",
    "- One time I discovered that the data contained duplicate examples. \n",
    "\n",
    "- Another time I found corrupted images / labels. \n",
    "\n",
    "- I look for data imbalances and biases. \n",
    "- I will typically also pay attention to my own process for classifying the data, which hints at the kinds of architectures weâ€™ll eventually explore.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Set up the end-to-end training/evaluation skeleton + get dumb baselines](https://karpathy.github.io/2019/04/25/recipe/#2-set-up-the-end-to-end-trainingevaluation-skeleton--get-dumb-baselines)\n",
    "\n",
    "Start with a simple model (like the Baseline models we suggest in the Recipe).\n",
    "\n",
    "The time to make naive mistakes is now, before you add complexity.\n",
    "\n",
    "Set up a process to make thing repeatable (i.e., the scientific process for experimentation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Acknowledge and control randomness\n",
    "\n",
    "Be aware of the sources of randomness in training and try to eliminate them.\n",
    "\n",
    "It's hard to debug/understand when each run is different\n",
    "- Shuffling of dataset\n",
    "- Random initialization of weights\n",
    "    - the *distribution* may be fixed, but not the samples from the distribution\n",
    "- Randomness you introduce explicitly: drawing random samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Both Tensorflow and Python random number generation is controlled by a (separate) seed value.\n",
    "\n",
    "Fixing this value for each run makes your program execution repeatable.\n",
    "\n",
    "More recent versions of TensorFlow provide a [`set_random_seed`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/set_random_seed) method\n",
    "- sets random seeds at multiple sources\n",
    "- equivalent to\n",
    "        import random\n",
    "        import numpy as np\n",
    "        import tensorflow as tf\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If your version of TensorFlow does not implement `set_random_seed` i have created an equivalent\n",
    "\n",
    "\n",
    "        def set_seed(seed, Debug=False):\n",
    "            try:\n",
    "                from tensorflow.keras.utils import set_random_seed\n",
    "                set_random_seed(seed)\n",
    "\n",
    "                if Debug:\n",
    "                    print(\"Used set_random_seed\")\n",
    "            except:\n",
    "                import random\n",
    "                import numpy as np\n",
    "                import tensorflow as tf\n",
    "                random.seed(seed)\n",
    "                np.random.seed(seed)\n",
    "                tf.random.set_seed(seed)\n",
    "\n",
    "                if Debug:\n",
    "                    print(\"Used individual setting of seeds\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Verify decreasing training loss\n",
    "\n",
    "When training starts, the initial loss will be high.\n",
    "\n",
    "If a model is \"learning\", the *training* (in-sample) loss will\n",
    "- decrease rapidly in early epochs\n",
    "- continue to decrease\n",
    "    - not necessarily in a straight line\n",
    "- reach a minimum (potentially bumpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**If your training loss is not decreasing** in early epochs: something is wrong !\n",
    "- Visualize the inputs  and labels to the Neural Network directly\n",
    "    - is the input correct ?\n",
    "        - learn how to obtain mini-batches\n",
    "    - do the labels match the features ?\n",
    "- Is your network \"too small\" to learn ?\n",
    "    - try increasing the size of the network (e.g., number of units per layer)\n",
    "\n",
    "Note: training loss will often decrease *without* a corresponding decrease in validation (out of sample) loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Set the bias on the head layer\n",
    "\n",
    "**Note: I've never seen anyone do this, but it's a great interview question at the least !**\n",
    "\n",
    "The head layer $L$ is usually a Classifier or Regressor, implemented as a `Dense` layer.\n",
    "\n",
    "Dense layer $\\ll$ computes a dot product of weights $W_\\llp$ and layer inputs $\\y_{(\\ll-1)}$\n",
    "- weights for each unit $j$ of layer $\\ll$ consists of a single \"bias\" $b_{\\llp,j}$ and vector of $\\w_{\\llp, j}$ of non-bias weights\n",
    "    - our convention is $b_{\\llp,j} = \\W_{\\llp,j,0}$ and $\\w_{\\llp, j} = \\W_{\\llp,j, [1:]}$\n",
    "- so unit $j$ of layer $L$ computes\n",
    "\n",
    "$$\n",
    "\\y_{(L),j} = \\y_{(L-1)} \\cdot \\w_{(L), j} + b_{(L), j}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Suppose we initialize the non-bias weights $\\w_{(L), j}$\n",
    "- from a random distribution (e.g., Normal, Uniform)\n",
    "- with mean 0\n",
    "\n",
    "Then the Expected value of unit $j$ is equal to the bias $b_{(L),j}$\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\Exp{}\\y_{(L),j} & = & \\Exp{} \\left( \\y_{(L-1)} \\cdot \\w_{(L), j} + b_{(L), j} \\right) \\\\ \n",
    "& = & b_{(L), j} & \\text{since } \\Exp{}\\w_{(L),j,k} = 0  \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This suggests that a good value for initializing the bias is\n",
    "- $b_{(L)} = \\bar \\y$ for a Regression task\n",
    "    - $\\bar\\y$ is average $\\y^\\ip$ over all training examples\n",
    "    - we omit subscript $j$ from $b_{(L})$ since we assume a single regression output\n",
    "    - error for example $i$ would be\n",
    "    $$\n",
    "    \\y^\\ip - \\bar\\y\n",
    "    $$\n",
    "    which has expected value (over all $i$)  of $0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- $b_{(L),j} = \\log p_j$ for logit $j$ of a Classification task\n",
    "    - where $p_j$ is the probability (over the training set) of examples with the $j^{th}$ label\n",
    "    - $\\log p_j$ is the value of the logit corresponding to probability $p_j$\n",
    "    - the initial predicted probability distribution *for each example* matches the training distribution (across all examples)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Setting the bias manually may speed up training\n",
    "- initial epochs of training may be primarily to *learn* this bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Verify the loss\n",
    "\n",
    "We can manually calculate the training Loss after one epoch and compare it to the actual training loss.\n",
    "\n",
    "If the computed and actual losses are not close: perhaps our Neural Network is not computing what we thought\n",
    "- incorrect loss function\n",
    "- mismatched features and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Assuming that the model's predictions are uninformed, due to random initialization of all weights (including bias) with mean $0$\n",
    "- Regressor  expected to predict near $0$ values\n",
    "    - so per-example error is $\\y^\\ip$\n",
    "    - translate error into Loss $\\loss^\\ip$ depending on Loss function (e.g., MSE, MAE)\n",
    "- Classifier expected to predict equal probability for each class\n",
    "    - logit value of $\\log\\frac{1}{\\text{number of classes}}$ for each class $j$\n",
    "    - corresponding to equal probability across classes that label is class $j$\n",
    "    - Loss is negative of this value: $\\loss^\\ip = - \\log\\frac{1}{\\text{number of classes}}$ \n",
    "        - we are *minimizing* loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overfit \n",
    "\n",
    "One danger in training a large Neural Network with a small number of examples is overfitting\n",
    "- Low training loss\n",
    "    - model has used the overly large number of weights to memorize the training set\n",
    "- High validation loss\n",
    "\n",
    "We can take advantage of this property to gain confidence that our Neural Network is \n",
    "performing the desired task\n",
    "- fit the model on a small subset of the Training examples\n",
    "- expect near $0$ loss.  If not\n",
    "    - mismatched  features and labels ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Regularize](https://karpathy.github.io/2019/04/25/recipe/#4-regularize)\n",
    "\n",
    "Regularization is often used to minimize the chances of overfitting\n",
    "- improve out of sample prediction\n",
    "\n",
    "Regularization is best performed *after* you have already successfully fit a model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Tune](https://karpathy.github.io/2019/04/25/recipe/#5-tune)\n",
    "\n",
    "Use random search, rather than grid search, \n",
    "for tuning hyper-paramters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Ensembles](https://karpathy.github.io/2019/04/25/recipe/#6-squeeze-out-the-juice)\n",
    "\n",
    "Ensembing (running a cohort of models) works for Deep Learning in the same way\n",
    "as in Classical Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
